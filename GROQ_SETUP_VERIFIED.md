# Groq API Setup - Verified ‚úÖ

## Summary
Your code is now configured to work with both Ollama and Groq using **identical prompts and functionality**.

## What Was Updated
1. ‚úÖ **API Key Updated** in `start_with_groq.sh`
   - API key configured in the script

## Verification: Same Prompts for Both Providers

### System Prompt (Identical for both Ollama and Groq)
Both `OllamaClient` and `GroqClient` use the **exact same system prompt**:

```
You are a helpful AI assistant with access to information from documents. 
Your task is to answer questions based on the provided context.

Guidelines:
1. Use the context provided to answer questions accurately
2. Write in a natural, conversational style
3. Use simple numbers for citations like [1], [2], etc.
4. Place citations at the end of sentences or paragraphs, not in the middle
5. After your main answer, add a "Sources:" section listing the citations
6. If the context doesn't contain relevant information, say so honestly
7. Answer based solely on the provided context - do not make up information
```

**Source Files:**
- `src/llm/ollama_client.py` (lines 57-94)
- `src/llm/groq_client.py` (lines 50-87)

### How Provider Switching Works

The system automatically switches between providers using the `LLM_PROVIDER` environment variable:

**Ollama (Local):**
```bash
./start_with_ollama.sh
# Sets: LLM_PROVIDER=ollama
```

**Groq (Cloud API):**
```bash
./start_with_groq.sh
# Sets: LLM_PROVIDER=groq
# Sets: GROQ_API_KEY (configured in script)
```

### What's the Same Between Both Providers

‚úÖ **Identical System Prompts** - Both use same instructions and guidelines  
‚úÖ **Same Citation Style** - Both use "clean" citations by default  
‚úÖ **Same RAG Pipeline** - Retrieval, context formatting, and response generation  
‚úÖ **Same Streaming** - Both support streaming responses  
‚úÖ **Same Chat History** - Both maintain conversation context  
‚úÖ **Same Temperature** - Both use 0.7 by default  
‚úÖ **Same Max Tokens** - Both use 2048 tokens  

### Implementation Details

The provider switching is handled in `backend/services/rag_service.py`:

```python
if llm_provider == "groq":
    logger.info("Using Groq API for LLM (fast inference)")
    self.llm_client = GroqClient(
        api_key=config.groq_api_key,
        model_name="llama-3.1-8b-instant",
        temperature=config.llm_config['temperature'],
        max_tokens=config.llm_config['max_tokens']
    )
else:  # Default to ollama
    logger.info("Using local Ollama for LLM")
    self.llm_client = OllamaClient(
        model_name=config.llm_config['model_name'],
        base_url=config.llm_config['base_url'],
        temperature=config.llm_config['temperature'],
        max_tokens=config.llm_config['max_tokens']
    )
```

## Models Used

| Provider | Model | Speed | Cost |
|----------|-------|-------|------|
| **Ollama** | llama3.1 | Medium | Free (Local) |
| **Groq** | llama-3.1-8b-instant | Ultra-fast | ~$0.05-0.10 per 1M tokens |

## Testing the Setup

### Test with Groq:
```bash
./start_with_groq.sh
```

Expected output:
```
üöÄ Starting Voice Chat with Groq API...
‚úÖ Starting backend with Groq API...
‚úÖ Starting frontend...
üéâ Voice Chat Running!
üì± Open: http://localhost:3000
‚ö° Using: Groq API (ultra-fast responses!)
```

### Test with Ollama:
```bash
./start_with_ollama.sh
```

Expected output:
```
üöÄ Starting Voice Chat with local Ollama...
‚úÖ Starting backend with local Ollama...
‚úÖ Starting frontend...
üéâ Voice Chat Running!
üì± Open: http://localhost:3000
üè† Using: Local Ollama (private, no API costs)
```

## Requirements

The `groq` package is already included in:
- ‚úÖ `requirements.txt` (groq==0.4.1)
- ‚úÖ `requirements-cpu.txt` (groq==0.4.1)
- ‚úÖ `requirements-minimal.txt` (groq)
- ‚úÖ `requirements_voice.txt` (groq)

## Troubleshooting

### If Groq doesn't connect:
1. Check API key is correct in `start_with_groq.sh`
2. Ensure you have internet connection (Groq is cloud-based)
3. Verify groq package is installed: `pip install groq==0.4.1`

### If Ollama doesn't connect:
1. Make sure Ollama is running: `ollama serve`
2. Verify model is installed: `ollama list`
3. Pull model if needed: `ollama pull llama3.1`

## Conclusion

‚úÖ **Your code is fully configured to work with both providers**  
‚úÖ **Same prompts, same functionality, same output format**  
‚úÖ **Just run the appropriate script to switch providers**  

No additional changes needed!

