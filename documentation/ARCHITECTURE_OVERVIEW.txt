================================================================================
STUDIO TOURS AI ASSISTANT - HIGH-LEVEL ARCHITECTURE OVERVIEW
================================================================================

PROJECT: Voice-Enabled RAG System for Warner Bros Studio Tours
EXCLUDES: Text-to-Speech, Groq API (as requested)

================================================================================
SYSTEM ARCHITECTURE LAYERS
================================================================================

1. FRONTEND LAYER (User Interface)
   Technology: Next.js + React + TypeScript
   ----------------------------------------
   Components:
   - Chat UI: Main conversational interface for text interaction
   - Voice Recorder: Captures user voice input for transcription
   - Settings Panel: Configurable RAG parameters (top-k, thresholds, etc.)
   - WebSocket Client: Real-time bidirectional communication with backend
   
   Communication: HTTP REST + WebSocket for streaming responses


2. BACKEND API LAYER (FastAPI)
   Technology: Python + FastAPI
   ----------------------------------------
   Components:
   - REST Endpoints: Standard HTTP endpoints for health checks, settings
   - WebSocket Handler: Manages real-time streaming chat connections
   - Transcription Service: Speech-to-text using OpenAI Whisper (local)
   - Session Manager: Manages user sessions and conversation state
   
   Responsibilities:
   - Request routing and validation
   - WebSocket connection management
   - Audio transcription coordination
   - Response streaming to frontend


3. RAG PIPELINE (Core Intelligence)
   Technology: Custom Python modules
   ----------------------------------------
   Components:
   
   a) Query Enhancer
      - Expands and enriches user queries with context
      - Improves retrieval accuracy
   
   b) Hybrid Retrieval
      - Vector Search (Semantic): Embedding-based similarity search
      - BM25 Search (Keyword): Traditional keyword-based search
      - RRF Fusion: Combines both methods using Reciprocal Rank Fusion
   
   c) Cross-Encoder Reranker
      - Re-scores retrieved candidates for accuracy
      - Uses: ms-marco-MiniLM-L-6-v2 model
   
   d) Context Formatter
      - Formats retrieved chunks into structured context
      - Adds source citations and metadata
   
   e) Memory Manager
      - Maintains conversation history (last 5 turns)
      - Enables context-aware responses
   
   Flow:
   User Query -> Query Enhancement -> Hybrid Retrieval (Vector + BM25) -> 
   Cross-Encoder Reranking -> Context Formatting -> LLM Generation


4. DATA STORAGE LAYER
   ----------------------------------------
   a) ChromaDB (Vector Store)
      - Stores document embeddings (384 dimensions)
      - Enables semantic similarity search
      - Persistent storage on disk
      
   b) BM25 Index (Keyword Search)
      - Keyword-based sparse retrieval
      - Complements vector search
      - Handles exact term matching


5. LANGUAGE MODEL LAYER
   Technology: Ollama (Local LLM Runtime)
   ----------------------------------------
   Model: Llama 3.1 8B (8 billion parameters)
   
   Capabilities:
   - Natural language understanding
   - Context-aware response generation
   - Streaming token generation
   - Multi-language support
   
   Deployment: 100% local (no external API calls)


6. DATA INGESTION PIPELINE (Offline Processing)
   ----------------------------------------
   Flow:
   PDF Documents -> Chunking -> Embedding Generation -> 
   Vector Store Ingestion -> BM25 Index Building
   
   Components:
   - PDF Documents: Source knowledge base (12 PDFs about studio tours)
   - Chunking: Recursive text splitting strategy
   - Embedding Service: Sentence Transformers (all-MiniLM-L6-v2)
   - Vector Store Ingestion: Batch insertion into ChromaDB
   - BM25 Index Building: Creates keyword search index


================================================================================
KEY ARCHITECTURAL FEATURES
================================================================================

1. HYBRID SEARCH APPROACH
   - Combines semantic (vector) and keyword (BM25) search
   - Reciprocal Rank Fusion (RRF) for result combination
   - Provides better accuracy than single-method retrieval

2. TWO-STAGE RETRIEVAL
   - Stage 1: Initial retrieval (larger candidate set ~25 documents)
   - Stage 2: Cross-encoder reranking (refined top ~5 documents)
   - Balances recall and precision

3. STREAMING RESPONSES
   - Token-by-token streaming from LLM to user
   - Reduces perceived latency
   - Better user experience with immediate feedback

4. CONVERSATION MEMORY
   - Maintains last 5 conversation turns
   - Context-aware query enhancement
   - Coherent multi-turn conversations

5. 100% LOCAL PROCESSING
   - All models run on local hardware
   - Zero external API dependencies (Ollama for LLM)
   - Complete data privacy and control


================================================================================
DATA FLOW (User Query Processing)
================================================================================

1. User speaks/types a question
   └─> Frontend captures input

2. Voice audio (if applicable) sent to Backend
   └─> Transcription Service converts to text (Whisper)

3. Text query sent via WebSocket to Backend
   └─> RAG Service processes query

4. Query Enhancement
   └─> Adds conversation context, expands query

5. Hybrid Retrieval
   ├─> Vector Search: ChromaDB returns top semantic matches
   ├─> BM25 Search: Keyword index returns top keyword matches
   └─> RRF Fusion: Combines both result sets

6. Cross-Encoder Reranking
   └─> Re-scores candidates, selects top 5

7. Context Formatting
   └─> Formats retrieved chunks with sources

8. LLM Generation (Ollama)
   └─> Generates response with context
   └─> Streams tokens back to user

9. Frontend displays streaming response
   └─> User sees answer with citations in real-time


================================================================================
TECHNOLOGY STACK SUMMARY
================================================================================

Frontend:
- Framework: Next.js 14 (React)
- Language: TypeScript
- Styling: Tailwind CSS
- Real-time: WebSocket client

Backend:
- Framework: FastAPI (Python)
- Async: Uvicorn ASGI server
- Speech-to-Text: OpenAI Whisper (local)

RAG Engine:
- Vector DB: ChromaDB
- Embeddings: Sentence Transformers (all-MiniLM-L6-v2, 384-dim)
- Keyword Search: BM25 (rank-bm25 library)
- Reranker: Cross-Encoder (ms-marco-MiniLM-L-6-v2)

LLM:
- Runtime: Ollama
- Model: Llama 3.1 8B Instruct
- Inference: Local (CPU/GPU)

Data Processing:
- Document Parsing: PyPDF2/PDFPlumber
- Chunking: Recursive character splitter
- Embedding Generation: Batch processing with sentence-transformers


================================================================================
SCALABILITY & PERFORMANCE NOTES
================================================================================

- Response Time: < 3 seconds for typical queries
- Concurrent Users: Supports multiple WebSocket connections
- Knowledge Base: 2000+ document chunks indexed
- Vector Dimensions: 384 (optimized for speed/accuracy balance)
- Hybrid Search: Improves accuracy by 15-20% over single-method retrieval
- Local Processing: No network latency for model inference


================================================================================
DEPLOYMENT CONSIDERATIONS
================================================================================

Hardware Requirements:
- Minimum 8GB RAM (16GB recommended)
- 10GB disk space (for models and vector DB)
- CPU: Multi-core recommended
- GPU: Optional (accelerates LLM inference)

Software Dependencies:
- Python 3.9+
- Node.js 18+
- Ollama runtime
- FFmpeg (for audio processing)


================================================================================
END OF ARCHITECTURE OVERVIEW
================================================================================
Generated for PPT presentation purposes
Excludes: Text-to-Speech features, Groq API references







